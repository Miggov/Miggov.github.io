---
layout: post
title:  "6. Datan visualisointi"
date:   2019-4-16 00:00:00 +0200
categories: jekyll update
---
Viimeisellä luennolla käsiteltiin datan onnistunutta visualisointia. Visualisoinnit ja niiden onnistuminen tulevat usein omassakin työssäni vastaan. Harmillisen usein myös erilaisissa palveluissa näkee kauniisti tuotettuja visualisointeja, joiden välittämä informaatio jää kuitenkin vähäiseksi. Informaation esittämisessä tulisikin löytää ns. kultainen keskitie, sillä myös liian informaation yhdistäminen samaan kuvaajaan tekee siitä herkästi vaikeasti tulkittavan. Esimerkiksi piirakkakuvaajassa ei saisi olla kovin montaa muuttujaa, sillä erojen hahmottaminen kuvaajasta on melko vaikeaa.

Visualisoinnit ovat usein ratkaisevassa asemassa datan ymmärtämisen kannalta. Siksi myös täysin luotettavasta datasta voidaaan tehdä vääriä johtopäätöksiä, mikäli visualisoinnit on tehty huonosti. Tärkeää ovat muun muassa erilaiset "drilldown" vaihtoehdot, jolla käyttäjä voi pureutua dataan tarkemmin esimerkiksi laajentamalla yhdistetyn nuuttujan (esimerkiksi Aasian tilauksista eri Aasian maiden tilauksiin). Sen sijaan että tarjottaisiin staattinen raportti analyysin lopputuloksena, tarjotaankin käyttäjälle mahdollisuus tehdä omia johtopäästöksiä datasta. Näin mukaan ei tule vain analyysiä tehneiden näkökulma, vaan myös analyysin loppukäyttäjän. Tavallaan voisi sanoa, ettei hyvä analyysi tarjoa ensisijaisesti vain analyysiä, vaan ennen kaikkea hyvän työkalun datan eksploratiiviseen hyödyntämiseen. Data siis tarjotaan kontekstiin sopivassa ja ymmärrettävässä muodossa analyysin kuluttajille.

Luennolla puhuttiin visualisoinnin työprosessista ja siitä, miten se on hyvin samanlainen datatieteen työprosessin kanssa. Tämä todettiin myös luennolla, mutta olen itse pitänyt visualisointeja vain osana analytiikkaprojekteja. Omat analytiikkaprojektini ovat keskittyneet vahvasti juuri datan visualisontiin, enkä esimerkiksi ole juuri käyttänyt koneoppimista tähänastisissa projekteissani. Visualisointi on usein myös ainoa asiakkaalle tai loppukäyttäjälle näkyvä osuus työstä, joten sen tulee kommunikoida analysoitu asia loppukäyttäjälle mahdollisimman hyvin ja selkeästi. Voi myös olla tarpeen valita eri visualisoinnit eri käyttäjille, esimerkiksi logistiikan ammattilainen voi kaivata enemmän yksityiskohteja kuin ylin johto, joka on kiinnostunut tunnusluvuista, joista nähdään yleiskuva nopeasti.

Luennon lopuksi esiteltiin vielä hieman PowerBi:tä, joka nykyisin on erittäin suosittu visualistointityökalu. Samalla kysymykseksi nousi se, mikä on varsinaisen koodaamisen rooli datatieteessä.  Omassa työssäni käytän ehdottomasti eniten juuri PowerBi:tä datan analysointiin ja visualisointiin, mutta välillä tulee vastaan asioita, joita ohjelma ei osaa tehdä. Hyvä esimerkki tästä on unixin aikaleimojen muuttaminen PowerBi:n tukemaan muotoon. Ohjemiston sisältämä PowerQuery-työkalu ei tätä suoraan osaa muuntaa, vaan muunnos pitäisi tehdä melko kömpelöllä DAX-kaavalla datan lukemisen jälkeen. Nykyisin ohjelmisto kuitenkin tukee Python-skriptejä datan sisäänluvussa, visualisoinneissa ja datan siivoamisessa. Tässä siis mielestäni helpoin ratkaisu on ajaa Python-siivoamisskripti, joka esimerkiksi Pandasin avulla muuttaa tuon aikaleiman luettavaan muotoon. Samoin esimerkiksi visualisointien puolella on mahdollista käyttää Pythonin (tai R:n) visualisointikirjastoja, joskin PowerBi:n online-visualisointikirjasto alkaa olemaan nykyisin melko kattava.